{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import data_cleaning as dc\n",
    "import analytics as ana\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns', 999)\n",
    "d1 = {a: a for a in xrange(24)}\n",
    "d2 = {a: a-1 for a in xrange(25, 113)}\n",
    "correction_mapping = dict(d1.items()+d2.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get players dataframe, this gives me all hero picks\n",
    "# Filling NaNs with 0 makes perfect sense here\n",
    "# All NaNs basically say that a player did not commit a certain action\n",
    "players_df = pd.read_csv('dota-2-matches/players.csv').fillna(0)\n",
    "# Get hero names, which allows me to parse nice names for my hero selection dataframe\n",
    "heros_chart = pd.read_csv('dota-2-matches/heros_chart_corrected.csv')\n",
    "# Get match info, mainly used for getting target labels for now\n",
    "games_df = pd.read_csv('dota-2-matches/match.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "players_df = dc.assign_player_team(players_df)\n",
    "hero_selection_df = dc.construct_hero_selection_df(players_df, heros_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = hero_selection_df.values\n",
    "y = games_df.radiant_win.astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "The Logisitic Regression Model performed better than I thought. The OG study performed 10% better, but then again it only used the ranked matches from the top 8% players. This is good news for me. I believe that the performance decrease is very much likely caused by the interaction between hero selections and other features within the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for Logit:  0.600525296242\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "print 'CV score for Logit: ', cross_val_score(lr, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "I am surprised that the Random Forest Classifier did not perform as well as Logistic Regression, given its excellent track record with my case studies. However, given the my featuer construction, and the way DotA 2 is played, Random Forest can be the sub-optimal classifier when I only hero selection as my feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for RF:  0.581874919769\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=150, criterion='entropy', n_jobs=-1)\n",
    "print 'CV score for RF: ', cross_val_score(rf, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "This, following the results from Logistic Regression, is somewhat expected. The OG study had KNN performing marginally better than the Logistic Regression. However, KNN is merely better than chance here. That, again, lends validity to interactions between hero selections and user/game features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for KNN:  0.528375556609\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "print 'CV score for KNN: ', cross_val_score(knn, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Models\n",
    "Allow me to preface this section by ditching KNN models for future testing. As I add more features beyond what I have so far, KNN will perform worse in prediction both time-wise and accuracy-wise. Adding features makes the distance calculation even harder. Moreover, any distanced based method is prone to the curse of dimensionality. Therefore, KNN seems like a less reasonable choice. You will not see any KNN beyond this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Engineered Feature(s): 10 minute benchmark for every player\n",
    "I believe that, by cutting off my observation at the 10 minute benchmark, I am preventing leaks. My job is essentially predicting the outcome of the endgame by looking at the first 10 mintues within the game. This does not violate that condition I imposed upon my study. The benchmark consists of the max running gold, number of last hits and experience level before the 10-minute mark. Luckily these can all be calculated from a split-apply-max algorithm. The choice of max RUNNING average is intentional, as the gold amount recorded in player_time.csv is the the gold owned at the recorded time. However, early games involves lots of purchases. A higher max running gold amount implies that a player could potentially purchase better items within that time period. Using the gold exactly AT the 10-minute mark will not necessarily capture that information. Experience and number of last hits are non-decreasing, hence choosing their running max is equivalent to getting the 10-minute mark record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This dataframe gives gold, last hits, experience at 60 seconds intervals\n",
    "players_time_df = pd.read_csv('dota-2-matches/player_time.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct the time bracket data, which cuts off everything after the time threshold\n",
    "ten_min_player_df = dc.construct_x_seconds_df(players_time_df, threshold=600)\n",
    "# Save the dataframe for future use, also saves memory usage\n",
    "ten_min_player_df.to_csv('dota-2-matches/ten_min_player_time.csv', index=False)\n",
    "# Construct the player info exactly at the 10 minute mark\n",
    "ten_min_max_wealth = dc.construct_x_seconds_max_wealth(ten_min_player_df)\n",
    "# Combine that info with my hero_selection_df\n",
    "first_adv_feature_mat = hero_selection_df.join(ten_min_max_wealth)\n",
    "X = first_adv_feature_mat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic regression benefitted quite a bit from the addition of 10-minute benchmarks of every player. It goes to show that even the first ten minutes of a game can be quite telling about the outcome of a game. While I try to fit the same feature matrix with a random forest classifier, I would check for other information that can be extracted from the first 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for Logit:  0.686724754547\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "print 'CV score for Logit: ', cross_val_score(lr, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Again, random forest performed worse than logistic regression. The addition of time info does not capture all interactions between player skills and hero selection. Since the added benchmarks consist of information on gold, last hits and experience for all 10 players within a match, splitting on a single player's information probably would not accomplish much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for RF:  0.651875546992\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=150, criterion='entropy', n_jobs=-1)\n",
    "print 'CV score for RF: ', cross_val_score(rf, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Engineered Feature(s): 10 minute gold growth benchmarks\n",
    "My motivation for investigating gold growth is two-fold. First, the first 10 minutes of the game was actually richer than I thought. I want to investigate information provided within this timeframe even more. Second, the max is a point estimate. It is not necessarily representative of the flow of information. Hence I am calcualting the mean and standard deviation of gold growth (calculated on a minute-to-minute basis) for each player within the first 10 minutes. The point gold calculation will be normalized by -100gold/min, since this is the gold growth even if a player stays idle. Mean and standard deviation will be calculated after the normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct the gold growth benchmark within the first 10 minutes of the game for every player\n",
    "# The gold_growth_benchmark contains both mean and standard deviation of gold growths\n",
    "gold_growth_benchmark = dc.construct_x_seconds_gold_growth_benchmark(ten_min_player_df)\n",
    "# Combine the info with the first advanced feature matrix from above\n",
    "second_adv_feature_mat = first_adv_feature_mat.join(gold_growth_benchmark)\n",
    "X = second_adv_feature_mat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "I guess it should not come as a surprise that logistic regression failed to perform any better than before. The previous max_wealth dataframe, which contains the max gold, number of last hits and experience, has told most of the story already. It seems that there are not much to be gained from the new metrics, which deals mainly with gold growth for each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for Logit:  0.689149394655\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "print 'CV score for Logit: ', cross_val_score(lr, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random Forest performed even worse than before. Again, it should be due to the fact that features are too decentralized for decision trees to make any meaningful splits. In that sense, the myriad of features actually negatively affects the accuracy of the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for RF:  0.647150091138\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=150, criterion='entropy', n_jobs=-1)\n",
    "print 'CV score for RF: ', cross_val_score(rf, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Engineered Feature(s): net death counts from team fights before the 10-minute mark\n",
    "The motivation for this feature is to attack information other than the amount of gold owned by each player within the first 10 minutes. For that I turn to teamfight information within the first 10 mintues. I want to get the death count for both radiant and dire before the 10 minute mark. Although death of an enemy player rewards the killer and nearby teammates with gold, death itself represents information about the interaction between teams, whose outcome does not rely solely on gold difference in early game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleted to save memory, also because second advanced feature matrix did not do well at all\n",
    "del second_adv_feature_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "team_fights_df = pd.read_csv('dota-2-matches/teamfights.csv')\n",
    "num_team_fights_before_ten_min_df = dc.construct_num_team_fights(team_fights_df)\n",
    "# Get the number of team fights before 10 minute mark for every match from above\n",
    "# Get information on those team fights down here\n",
    "teamfight_players_df = pd.read_csv('dota-2-matches/teamfights_players.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extremely heavy computation, DAMN YOU FOR LOOPS\n",
    "net_death_count_before_ten_min_df = dc.construct_net_death_count_from_teamfights(teamfight_players_df, \n",
    "                                                                                 num_team_fights_before_ten_min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saved for future use, also because the current implementation of the function is very inefficient\n",
    "# Will improve later\n",
    "net_death_count_before_ten_min_df.to_csv('dota-2-matches/net_death_count_before_ten_min.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "third_adv_feature_mat = first_adv_feature_mat.join(net_death_count_before_ten_min_df)\n",
    "X = third_adv_feature_mat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "At this point, I believe my model's performance is being bottlenecked. Logistic regression performed more or less on the same level as what we did before. The informatin space of the first 10 minutes of every game is being exhausted, resulting in the meagre improvement in cross validation accuracy. If random forest fails to yield any huge improvement, it should corroborate this very claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for Logit:  0.685724434303\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "print 'CV score for Logit: ', cross_val_score(lr, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Here ends my search for features within the first 10 minutes of all matches. Random forest did not see a significant improvement to its performance either. From this point on, I will attempt to go back to game-based features. My next feature will be the team compositions. Since each hero takes on different roles, a team's composition can be potentially useful for predicting match outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for RF:  0.649798619947\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=150, criterion='entropy', n_jobs=-1)\n",
    "print 'CV score for RF: ', cross_val_score(rf, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4th Engineered Feature(s): Role Compositions\n",
    "The motivation for this feature is simple. Every hero takes on different roles. If hero selection matters, then it should not just be about counters, but also how each hero operates throughout the game. Following this line of logic, I am going to first scrape the official DotA 2 website for hero roles and construct a hero attribute dataframe, then get the hero composition for each team in every game and see if my model gets higher accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The hero index mapping is a dictionary that has hero name as keys and index as values\n",
    "hero_index_mapping = dc.get_hero_index_mapping(heros_chart)\n",
    "# Scrape the website for hero role information, and return a dictionary with hero names as keys and their list of roles as values\n",
    "hero_roles = dc.construct_hero_roles(hero_index_mapping)\n",
    "# Compile all possible roels as features, give each hero 1 for taking on a role, and 0 otherwise\n",
    "hero_attributes = dc.construct_hero_attribute_df(hero_roles)\n",
    "# For each team in every game, sum hero attributes for selected heros (hence max value in any team role is 5)\n",
    "# Roles in considerations are: Carry, Support, Disabler, Initiator. The rest are dropped during construction.\n",
    "hero_composition_df = dc.construct_hero_composition_df(players_df, hero_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It makes sense to have interaction between carries and supports, so this cell handles that\n",
    "# Gives two new columns, each representing the quantity of support per unit of carry\n",
    "hero_composition_df['Carry_Support_radiant'] = hero_composition_df['Support_radiant'] / (hero_composition_df['Carry_radiant']+0.01)\n",
    "hero_composition_df['Carry_Support_dire'] = hero_composition_df['Support_dire'] / (hero_composition_df['Carry_dire']+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fourth_adv_feature_mat = first_adv_feature_mat.join(hero_composition_df)\n",
    "X = fourth_adv_feature_mat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "This is actually pretty disappointing. My Logistic Regression's performance is still bottlenecked. At this point, I believe that I am not using my domain knowledge of the game enough. DotA 2's official role assignment is not necessarily accurate. Almost every hero takes on more than 3 roles. The separation of duties is not as clear-cut as the model would like it to be. Moreover, I assigned 1 for simply taking on a role. However, in reality these each hero takes on roles with differing degrees. My current assignment fails to capture that. I will hard code a version of hero role information based on domain knowledge and a bit of research to address this issue in the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for Logit:  0.689724967356\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(penalty='l1', C=10)\n",
    "print 'CV score for Logit: ', cross_val_score(lr, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "The Random Forest continues to perform worse than the Logistic Regression. I've mentioned 2 potential problems above. Let's try to address them next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for RF:  0.650675572452\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=150, criterion='entropy', n_jobs=-1)\n",
    "print 'CV score for RF: ', cross_val_score(rf, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the hardcoded version of hero_attributes based on domain knowledge and a bit of reasearch into each hero\n",
    "# Every hero has its role values sum up to 1 (still, max value in a role for each team is 5)\n",
    "# However we do have better handle on how deep into a role a hero is\n",
    "hero_attribute_df = dc.construct_hard_coded_hero_attribute_df()\n",
    "hero_composition_df = dc.construct_hero_composition_df(players_df, hero_attribute_df)\n",
    "# Again, interaction terms are worth looking into\n",
    "hero_composition_df['Carry_Support_radiant'] = hero_composition_df['Carry_radiant'] * hero_composition_df['Hard_Support_radiant']\n",
    "hero_composition_df['Carry_Support_dire'] = hero_composition_df['Carry_dire'] * hero_composition_df['Hard_Support_dire']\n",
    "fourth_adv_feature_mat = first_adv_feature_mat.join(hero_composition_df)\n",
    "X = fourth_adv_feature_mat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for Logit:  0.688225560961\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(penalty='l1', C=10)\n",
    "print 'CV score for Logit: ', cross_val_score(lr, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for RF:  0.651375\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=150, criterion='entropy', n_jobs=-1)\n",
    "print 'CV score for RF: ', cross_val_score(rf, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Firstblood status\n",
    "I am only looking at first kills that takes place within the first 10 minutes. In pro scenes, first bloods usually do not carry much implications. In lower brackets, however, a first kill can affect players' mood and can possibly affect the outcome of the game. This is my motivation for looking into firstbloods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objectives_df = pd.read_csv('dota-2-matches/objectives.csv').drop('key', axis=1)\n",
    "ten_min_firstblood = dc.construct_ten_min_firstblood_df(objectives_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fifth_adv_feature_mat = ten_min_firstblood.join(fourth_adv_feature_mat, on='match_id', how='right')\\\n",
    "                        .sort_values(by='match_id').set_index('match_id').fillna(-1)\n",
    "dummies = pd.get_dummies(fifth_adv_feature_mat.radiant_firstblood, prefix='firstblood')\\\n",
    "          .drop('firstblood_-1.0', axis=1).rename(columns={'firstblood_1.0':'Radiant_Firstblood',\n",
    "                                                           'firstblood_0.0':'Dire_Firstblood'})\n",
    "fifth_adv_feature_mat = fifth_adv_feature_mat.join(dummies).drop('radiant_firstblood', axis=1)\n",
    "X = fifth_adv_feature_mat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for Logit:  0.685700346703\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(penalty='l1', C=10, fit_intercept=False)\n",
    "print 'CV score for Logit: ', cross_val_score(lr, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra 2: Role Checks\n",
    "Check whether a team has too many carries (num >= 3). Give 1 if there are and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "game_hero_info = dc.get_game_hero_info(players_df, hero_attribute_df)\n",
    "role_checks = dc.construct_role_check_df(game_hero_info)\n",
    "sixth_adv_feature_mat = fourth_adv_feature_mat.join(role_checks)\n",
    "X = sixth_adv_feature_mat.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for Logit:  0.688374796191\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(penalty='l1', C=10)\n",
    "print 'CV score for Logit: ', cross_val_score(lr, X_train, y_train, scoring='accuracy', cv=10, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5th Engineered Feature(s): Interaction between Role and Gold\n",
    "Often it is likely the case that when a team's support is doing extremely well in the early game, the team will likely win the game. This offers a potential angle of investigation that is neither purely game based or player based. Rather, it is based on interactions between player skills and their heros' roles. Initial check is going to be looking at each hero's max gold difference from the mean max gold of all 10 players within the first ten minutes of a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "long_player_gold = dc.construct_long_player_gold_df(ten_min_max_wealth)\n",
    "hero_gold_info = game_hero_info.merge(long_player_gold, on=['match_id', 'player_slot'])\n",
    "hero_gold_avg_comp = hero_gold_info.groupby('match_id')['max_gold'].apply(lambda x: x-x.mean())\n",
    "role_gold_comp = hero_gold_info.join(hero_gold_avg_comp, rsuffix='_diff_from_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>hero_id</th>\n",
       "      <th>player_slot</th>\n",
       "      <th>radiant_player</th>\n",
       "      <th>role</th>\n",
       "      <th>max_gold</th>\n",
       "      <th>max_gold_diff_from_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Hard_Support</td>\n",
       "      <td>2211</td>\n",
       "      <td>-600.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Mid</td>\n",
       "      <td>3379</td>\n",
       "      <td>567.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>Hard_Support</td>\n",
       "      <td>1650</td>\n",
       "      <td>-1161.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>Carry</td>\n",
       "      <td>2859</td>\n",
       "      <td>47.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>Hard_Support</td>\n",
       "      <td>3745</td>\n",
       "      <td>933.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_id  hero_id  player_slot radiant_player          role  max_gold  \\\n",
       "0         0       85            0           True  Hard_Support      2211   \n",
       "1         0       50            1           True           Mid      3379   \n",
       "2         0       82            2           True  Hard_Support      1650   \n",
       "3         0       11            3           True         Carry      2859   \n",
       "4         0       66            4           True  Hard_Support      3745   \n",
       "\n",
       "   max_gold_diff_from_mean  \n",
       "0                   -600.5  \n",
       "1                    567.5  \n",
       "2                  -1161.5  \n",
       "3                     47.5  \n",
       "4                    933.5  "
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role_gold_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
